{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "# Import data\n",
    "from Preprocessing import X, Y, n_fold, cv, random_split \n",
    "trainX, trainY, testX, testY = random_split(0.8) # set aside 20% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation function\n",
    "def k_fold_cross_validation(MODEL, range_hyperparam:list, hyperparam_name:str, additional_params=None) -> np.array:\n",
    "    \"\"\"Performs k-fold cross validation for a given model and hyperparameter / range of hyperparameter\"\"\"\n",
    "    acc = np.zeros(len(range_hyperparam))\n",
    "    for train_index, val_index in cv.split(trainX): # Loopes over all folds\n",
    "        X_train, X_val = trainX.iloc[train_index], trainX.iloc[val_index]\n",
    "        Y_train, Y_val = trainY.iloc[train_index], trainY.iloc[val_index]\n",
    "\n",
    "        for j,k in enumerate(range_hyperparam): # Loopes over all hyperparameters\n",
    "            kwargs = {hyperparam_name: k}\n",
    "            if additional_params is not None:\n",
    "                kwargs.update(additional_params)\n",
    "            model = MODEL(**kwargs)\n",
    "            model.fit(X_train, Y_train)\n",
    "            acc[j] += model.score(X_val, Y_val)\n",
    "\n",
    "    acc /= n_fold # average of all average accuracies\n",
    "    return acc\n",
    "\n",
    "def best_hyperparam(acc, range_hyperparam, model, hyperparam_name, additional_params=None):\n",
    "    \"\"\"Returns the best hyperparameter based on the accuracy\"\"\"\n",
    "    best_hyperparam = range_hyperparam[np.argmax(acc)]\n",
    "    kwargs = {hyperparam_name: best_hyperparam}\n",
    "    if additional_params is not None:\n",
    "        kwargs.update(additional_params)\n",
    "    BestModel = model(**kwargs)\n",
    "    BestModel.fit(trainX, trainY)\n",
    "    \n",
    "    print('Accuracy with best hyperparameter: ', np.max(acc))\n",
    "    print('Best hyperparameter: ', best_hyperparam, '\\n')   \n",
    "    print('Confusion matrix:')\n",
    "    print(pd.crosstab(testY, BestModel.predict(testX), rownames=['True'], colnames=['Predicted'], margins=False))\n",
    "    pd.crosstab(testY, BestModel.predict(testX), rownames=['True'], colnames=['Predicted'], margins=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree\n",
    "1 tree different max depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 150)\n",
    "acc_gini = k_fold_cross_validation(tree.DecisionTreeClassifier, depths, 'max_depth')\n",
    "acc_entropy = k_fold_cross_validation(tree.DecisionTreeClassifier, depths, 'max_depth', {'criterion':'entropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths, 1- acc_gini, label='Gini')\n",
    "plt.plot(depths, 1- acc_entropy, label='Entropy')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Validation error for different depths and splitting criteria')\n",
    "plt.legend()\n",
    "plt.savefig('Figures/SingleTree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best depth tree with Gini:\\n')\n",
    "best_hyperparam(acc_gini, depths, tree.DecisionTreeClassifier, 'max_depth')\n",
    "print('\\n\\n')\n",
    "print(f'Best depth tree with Entropy:\\n')\n",
    "best_hyperparam(acc_entropy, depths, tree.DecisionTreeClassifier, 'max_depth', {'criterion':'entropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "Test different number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = range(1, 150)\n",
    "acc_bag = k_fold_cross_validation(BaggingClassifier, trees, 'n_estimators', {'n_jobs': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trees, 1 - acc_bag)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Bagging validation error for different number of trees')\n",
    "plt.savefig('Figures/Bagging.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best number of trees for bagging \\n')\n",
    "best_hyperparam(acc_bag, trees, BaggingClassifier, 'n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Random forest with different number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = range(1, 150)\n",
    "acc_randf = k_fold_cross_validation(RandomForestClassifier, trees, 'n_estimators', {'random_state': 0, 'n_jobs': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trees, 1 - acc_randf)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Random forest validation error for different number of trees')\n",
    "plt.savefig('Figures/RandomForest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best number of trees for random forest \\n')\n",
    "best_hyperparam(acc_randf, trees, RandomForestClassifier, 'n_estimators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "plt.plot(trees, 1 - acc_bag, label='Bagging')\n",
    "plt.plot(trees, 1 - acc_randf, label='Random forest')\n",
    "plt.plot(depths, 1 - acc_entropy, label='Decision tree')\n",
    "\n",
    "plt.xlabel('Number of trees/depth of tree')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Comparison of different models')\n",
    "plt.legend()\n",
    "plt.savefig('Figures/AllTrees.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the bagging and random forest models perform equal. The decision tree seem to have optimal performance if we use entropy split. Use this tree as the estimators for bagging and random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees2 = range(1, 150)\n",
    "bestTree = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "acc_bag2 = k_fold_cross_validation(BaggingClassifier, trees2, 'n_estimators', {'estimator': bestTree, 'n_jobs': -1})\n",
    "acc_randf2 = k_fold_cross_validation(RandomForestClassifier, trees2, 'n_estimators',{'criterion':'entropy', 'n_jobs': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trees, 1 - acc_bag, label='Bagging')\n",
    "plt.plot(trees, 1 - acc_randf, label='Random forest')\n",
    "plt.plot(trees2, 1 - acc_bag2, label='Bagging with best tree')\n",
    "plt.plot(trees2, 1 - acc_randf2, label='Random forest with best tree')\n",
    "\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Bagging and random forest with & without best tree')\n",
    "plt.xlim(0,150)\n",
    "plt.ylim(0.085, 0.15)\n",
    "plt.legend()\n",
    "plt.savefig('Figures/BestTree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best number of trees for bagging with best tree \\n')\n",
    "best_hyperparam(acc_bag2, trees2, BaggingClassifier, 'n_estimators', {'estimator': bestTree})\n",
    "print('\\n-----------------------------------------------------\\n')\n",
    "print('Best number of trees for random forest with best tree \\n')\n",
    "best_hyperparam(acc_randf2, trees2, RandomForestClassifier, 'n_estimators' , {'criterion':'entropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best tree seems to improve results a tiny bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model\n",
    "Final best model is the Bagging classifier with entropy splitting and approximately 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalModel = BaggingClassifier(n_estimators=100, estimator=bestTree, random_state=0)\n",
    "finalModel.fit(trainX, trainY)\n",
    "# Test the final model using the test set\n",
    "print('Accuracy of final bagging model: ', finalModel.score(testX, testY))\n",
    "print('Confusion matrix:')\n",
    "pd.crosstab(testY, finalModel.predict(testX), rownames=['True'], colnames=['Predicted'], margins=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
